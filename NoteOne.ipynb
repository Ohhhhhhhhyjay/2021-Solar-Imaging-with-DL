{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit"
  },
  "interpreter": {
   "hash": "f06167d67f7d333834d778cba2aba1bad99950d0b55c8f4f285a548d8beae30f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Learning Keras"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "source": [
    "## Data loading & preprocessing\n",
    "\n",
    "Neural networks don't process raw data, like text files, encoded JPEG image files, or CSV files. They process vectorized & standardized representations.\n",
    "\n",
    "Images need to be read and decoded into **integer tensors**, then converted to **floating point and normalized** (usually between 0 and 1).\n",
    "\n",
    "### Data loading\n",
    "\n",
    "Keras models accept 3 types of inputs: NumPy arrays, TensorFlow `Dataset` objects and Python generators that yield batches of data.\n",
    "\n",
    "Note: \n",
    "If you have a large dataset and you are training on GPU(s), consider using Dataset objects, since they will take care of performance-critical details. e.g. Preprocessing with CPU while GPU is busy. Prefetching data on GPU memory, fully utilizing GPU.\n",
    "\n",
    "`tf.keras.preprocessing.image_dataset_from_directory` turns image files sorted into calss-specific folders into a labeled dataset of image tensors."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_directory/\n",
    "# ...class_a/\n",
    "# ......a_image_1.jpg\n",
    "# ......a_image_2.jpg\n",
    "# ...class_b/\n",
    "# ......b_image_1.jpg\n",
    "# ......b_image_2.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a test data set. Transfer fits to jpg. Not sure if we can use fits directly. Data in jpg is incredibly small, don't know if there is  any quality loss.\n",
    "# need only to run once\n",
    "import sunpy.map\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "path = r'E:\\Researches\\2020EUVSolarFlare\\Data_test\\aia_test'\n",
    "j = 1\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        file = os.path.join(root, file)\n",
    "        map = sunpy.map.Map(file)\n",
    "        data = map.data\n",
    "        plt.imsave('test' + str(j) + '.jpg', data, cmap='gray')\n",
    "        j += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 10 files belonging to 2 classes.\n",
      "(5, 4096, 4096, 3)\n",
      "<dtype: 'float32'>\n",
      "(5,)\n",
      "<dtype: 'int32'>\n",
      "(5, 4096, 4096, 3)\n",
      "<dtype: 'float32'>\n",
      "(5,)\n",
      "<dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset.\n",
    "data_path = r'E:\\Program Files\\VSCode\\2021_ImagingWithDeepLearning\\my_test_data'\n",
    "dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "  data_path, batch_size=5, image_size=(4096, 4096))\n",
    "\n",
    "# For demonstration, iterate over the batches yielded by the dataset.\n",
    "for data, labels in dataset:\n",
    "    print(data.shape)  # (5, 4096, 4096, 3)\n",
    "    print(data.dtype)  # float32\n",
    "    print(labels.shape)  # (5,)\n",
    "    print(labels.dtype)  # int32"
   ]
  },
  {
   "source": [
    "The label of a sample is the rank of its folder in alphanumeric order. Naturally, this can also be configured explicitly by passing, e.g. `class_names=['class_a', 'class_b']`, in which cases label 0 will be `class_a` and 1 will be `class_b`.\n",
    "\n",
    "data的最后一个分量是什么意思？3维数据？"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Note: 这里jpg格式不好,它损失很多数据,还得自己写一个加载的函数比较好."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Directly we use:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((examples, labels))"
   ]
  },
  {
   "source": [
    "## Data preprocessing with Keras\n",
    "\n",
    "Once your data is in the form of string/int/float NumpPy arrays, or a Dataset object (or Python generator) that yields batches of string/int/float tensors, it is time to preprocess the data.\n",
    "\n",
    "### Using Keras preprocessing layers\n",
    "\n",
    "In Keras, you do in-model data preprocessing via **preprocessing layers**. For image it means image rescaling, cropping, or image data augmentation. Note that the key advantage of using Keras preprocessing layers is that they can be included directly into your model, either during training or after training, which makes your models portable.\n",
    "\n",
    "The state of a preprocessing layer is obtained by calling `layer.adapt(data)` on a sample of the training data (or all of it).\n",
    "\n",
    "#### Example: rescaling & center-cropping images\n",
    "\n",
    "Both the `Rescaling` layer and the `CenterCrop` layer are stateless, so it isn't necessary to call `adapt()` in this case."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shape: (5, 4096, 4096, 3)\n",
      "min: 0.0\n",
      "max: 1.0\n",
      "shape: (5, 4096, 4096, 3)\n",
      "min: 0.0\n",
      "max: 1.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import CenterCrop\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "\n",
    "for data, labels in dataset:\n",
    "    training_data = data\n",
    "\n",
    "    # cropper = CenterCrop(height=150, width=150)\n",
    "    scaler = Rescaling(scale=1.0 / 255)\n",
    "\n",
    "    output_data = scaler(training_data)\n",
    "    print(\"shape:\", output_data.shape)\n",
    "    print(\"min:\", np.min(output_data))\n",
    "    print(\"max:\", np.max(output_data))"
   ]
  },
  {
   "source": [
    "## Building models with the Keras Functional API\n",
    "\n",
    "A \"layer\" is a simple input-output transformation (such as the scaling & center-cropping transformations above). A \"model\" is a directed acyclic graph of layers. You can think of a model as a \"bigger layer\" that encompasses multiple sublayers and that can be trained via exposure to data.\n",
    "\n",
    "**Functional API** the most common and powerful way to build Keras models.\n",
    "1. specify the shape(and optionally the dtype) of your inpus. If any dimension of your input can vary, you can specify it as `None`. e.g. `(200, 200, 3)` for fixed shape or `(None, None, 3)` for arbitrary shape."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say we expect our inputs to be RGB images of arbitrary size\n",
    "inputs = keras.Input(shape=(4096, 4096, 3))"
   ]
  },
  {
   "source": [
    "After defining your input(s), you can chain layer transformations on top of your inputs, until your final output:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# Center-crop images to ?\n",
    "x = CenterCrop(height=150, width=150)(inputs)\n",
    "# Rescale images to [0, 1]\n",
    "x = Rescaling(scale=1.0 / 255)(x)\n",
    "\n",
    "# Apply some convolution and pooling layers\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=(3, 3))(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=(3, 3))(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "\n",
    "# Apply global average pooling to get flat feature vectors\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add a dense classifier on top\n",
    "num_classes = 10\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)"
   ]
  },
  {
   "source": [
    "Once you have defined the directed acyclic graph of layers that turns your input(s) into your outputs, instantiate a `Model` object:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "source": [
    "This model behaves basically like a bigger layer. You can call it on batches of data, like this:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(5, 10)\n",
      "(5, 10)\n"
     ]
    }
   ],
   "source": [
    "for data, labels in dataset:\n",
    "    processed_data = model(data)\n",
    "    print(processed_data.shape)"
   ]
  },
  {
   "source": [
    "This is a classifier model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 4096, 4096, 3)]   0         \n_________________________________________________________________\ncenter_crop_2 (CenterCrop)   (None, 150, 150, 3)       0         \n_________________________________________________________________\nrescaling_7 (Rescaling)      (None, 150, 150, 3)       0         \n_________________________________________________________________\nconv2d_9 (Conv2D)            (None, 148, 148, 32)      896       \n_________________________________________________________________\nmax_pooling2d_6 (MaxPooling2 (None, 49, 49, 32)        0         \n_________________________________________________________________\nconv2d_10 (Conv2D)           (None, 47, 47, 32)        9248      \n_________________________________________________________________\nmax_pooling2d_7 (MaxPooling2 (None, 15, 15, 32)        0         \n_________________________________________________________________\nconv2d_11 (Conv2D)           (None, 13, 13, 32)        9248      \n_________________________________________________________________\nglobal_average_pooling2d_3 ( (None, 32)                0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 10)                330       \n=================================================================\nTotal params: 19,722\nTrainable params: 19,722\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "source": [
    "The Functional API also makes it easy to build models that have multiple inputs (for instance, an *image and its metadata*) or multiple outputs (for instance, predicting the class of the image and the likelihood that a user will click on it). For a deeper dive into what you can do, see our guide to the Functional API."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Training models with `fit()`\n",
    "\n",
    "The next step is to train your model on your data. The `Model` class features a built-in training loop, the `fit()` method. It accepts `Dataset` objects, Python generators that yield batches of data, or NumPy arrays.\n",
    "\n",
    "Before you can call `fit()`, you need to specify an `optimizer` and a `loss function` (we assume you are already familiar with these concepts). This is the `compile()` step:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "              loss=keras.losses.CategoricalCrossentropy())"
   ]
  },
  {
   "source": [
    "Loss and optimizer can be specified via their string identifiers (in this case their default constructor argument values are used):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "source": [
    "Once your model is compiled, you can start \"fitting\" the model to the data. Here's what fitting a model looks like with NumPy data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(numpy_array_of_samples, numpy_array_of_labels,\n",
    "          batch_size=32, epochs=10)"
   ]
  },
  {
   "source": [
    "Besides the data, you have to specify two key parameters: the `batch_size` and the number of epochs (iterations on the data). Here our data will get sliced on batches of 32 samples, and the model will iterate 10 times over the data during training."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(dataset_of_samples_and_labels, epochs=10)"
   ]
  },
  {
   "source": [
    "Since the data yielded by a dataset is expected to be already batched, you don't need to specify the batch size here."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's look at it in practice with a toy example model that learns to classify MNIST digits:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "rescaling (Rescaling)        (None, 28, 28)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Fit on NumPy data\n",
      "938/938 [==============================] - 2s 1ms/step - loss: 0.2673\n",
      "Fit on Dataset\n",
      "938/938 [==============================] - 1s 1ms/step - loss: 0.1175\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "# Get the data as Numpy arrays\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Build a simple model\n",
    "inputs = keras.Input(shape=(28, 28))\n",
    "x = layers.experimental.preprocessing.Rescaling(1.0 / 255)(inputs)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
    "\n",
    "# Train the model for 1 epoch from Numpy data\n",
    "batch_size = 64\n",
    "print(\"Fit on NumPy data\")\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=1)\n",
    "\n",
    "# Train the model for 1 epoch using a dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "print(\"Fit on Dataset\")\n",
    "history = model.fit(dataset, epochs=1)"
   ]
  },
  {
   "source": [
    "The fit() call returns a \"history\" object which records what happened over the course of training. The `history.history` dict contains per-epoch timeseries of metrics values (here we have only one metric, the loss, and one epoch, so we only get a single scalar):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': [0.11748092621564865]}\n"
     ]
    }
   ],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "source": [
    "### Keeping track of performance matrics\n",
    "\n",
    "#### Monitoring metrics\n",
    "\n",
    "One can pass a list of metric objects to `compile()`\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
    ")\n",
    "history = model.fit(dataset, epochs=1)"
   ]
  },
  {
   "source": [
    "#### Passing validation data to `fit()`\n",
    "\n",
    "One can pass validation data to `fit()` to monitor your validation loss & validation metrics. Validation metrics get reported at the end of each epoch."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "history = model.fit(dataset, epochs=1, validation_data=val_dataset)"
   ]
  },
  {
   "source": [
    "### Using callbacks for checkpointing(and more)\n",
    "\n",
    "Heavy lifting calculations such as ours will need safety measures. **callbacks**, configured in `fit()`, as an important feature of Keras, are objects that get called by the model at different point during training. In particular at the *beginning and end of each batch, and each epoch*.\n",
    "\n",
    "''If training goes on for more than a few minutes, it's important to save your model at regular intervals during training. You can then use your saved models to restart training in case your training process crashes (this is important for multi-worker distributed training, since with many workers at least one of them is bound to fail at some point).''\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "You can use callbacks to periodically save your model. Here's a simple example: a `ModelCheckpoint` callback configured to save the model at the end of every epoch. The filename will include the current epoch."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='path/to/my/model_{epoch}',\n",
    "        save_freq='epoch')\n",
    "]\n",
    "model.fit(dataset, epochs=2, callbacks=callbacks)"
   ]
  },
  {
   "source": [
    "You can also use callbacks to do things like periodically changing the learning of your optimizer, streaming metrics to a Slack bot, sending yourself an email notification when training is complete, etc."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Monitoring training progress with `TensorBoard`\n",
    "\n",
    "TensorBoard, a web app that can display real-time graphs of your metircs(and more).\n",
    "\n",
    "To use TensorBoard with `fit()`, simply pass a `keras.callbacks.TensorBoard` callback specifying the directory where to store TensorBoard logs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "]\n",
    "model.fit(dataset, epochs=2, callbacks=callbacks)"
   ]
  },
  {
   "source": [
    "You can then launch a TensorBoard instance that you can open in your browser to monitor the logs getting written to this location:\n",
    "\n",
    "tensorboard --logdir=./logs\n",
    "\n",
    "What's more, you can launch an in-line TensorBoard tab when training models in Jupyter / Colab notebooks."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## After `fit()`: evaluating test performance & generating predictions on new data\n",
    "\n",
    "Once you have a trained model, you can evaluate its loss and metrics on new data via `evaluate()`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "loss, acc = model.evaluate(val_dataset)  # returns loss and metrics\n",
    "print(\"loss: %.2f\" % loss)\n",
    "print(\"acc: %.2f\" % acc)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "You can also generate NumPy arrays of predictions (the activations of the output layer(s) in the model) via `predict()`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(val_dataset)\n",
    "print(predictions.shape)"
   ]
  },
  {
   "source": [
    "This is what we would be doing."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Debugging with `run_eagerly=True`\n",
    "\n",
    "''With eager execution, the Python code you write is the code that gets executed.'' Use it every time you need to debug inside the `fit()` call."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse', run_eagerly=True)"
   ]
  },
  {
   "source": [
    "## Speeding up with multiple GPUs\n",
    "\n",
    "with `tf.distribute` API. If you have multiple GPUs on your machine, you can train your model on all of them by:\n",
    "\n",
    "1. Creating a `tf.distribute.MirroredStrategy` object\n",
    "2. Building & compiling your model inside the strategy's scope\n",
    "3. Calling `fit()` and `evaluate()` on a dataset as usual"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MirroredStrategy.\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# Open a strategy scope.\n",
    "with strategy.scope():\n",
    "  # Everything that creates variables should be under the strategy scope.\n",
    "  # In general this is only model construction & `compile()`.\n",
    "  model = Model(...)\n",
    "  model.compile(...)\n",
    "\n",
    "# Train the model on all available devices.\n",
    "train_dataset, val_dataset, test_dataset = get_dataset()\n",
    "model.fit(train_dataset, epochs=2, validation_data=val_dataset)\n",
    "\n",
    "# Test the model on all available devices.\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "source": [
    "## Finding the best model configuration with hyperparameter tuning\n",
    "\n",
    "To find best hyperparameter. Use `Keras Tuner`.\n",
    "\n",
    "First, place your model definition in a function, that takes a single `hp` argument. Inside this function, replace any value you want to tune with a call to hyperparameter sampling methods, e.g. `hp.Int()` or `hp.Choice()`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    inputs = keras.Input(shape=(784,))\n",
    "    x = layers.Dense(\n",
    "        units=hp.Int('units', min_value=32, max_value=512, step=32),\n",
    "        activation='relu'))(inputs)\n",
    "    outputs = layers.Dense(10, activation='softmax')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate',\n",
    "                      values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "source": [
    "The function should return a compiled model.\n",
    "\n",
    "Next, instantiate a *tuner object* specifying your optimization objective and other search parameters:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner\n",
    "\n",
    "tuner = keras_tuner.tuners.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_epochs=100,\n",
    "    max_trials=200,\n",
    "    executions_per_trial=2,\n",
    "    directory='my_dir')"
   ]
  },
  {
   "source": [
    "Finally, start the search with the `search()` method, which takes the same arguments as `Model.fit()`:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(dataset, validation_data=val_dataset)"
   ]
  },
  {
   "source": [
    "When search is over, you can retrieve the best model(s):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = tuner.get_best_models(num_models=2)"
   ]
  },
  {
   "source": [
    "Or print a summary of the results:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  }
 ]
}